{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0987f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'The likeness of those who disbelieve in their Lord: their works are like ashes, in a fierce wind, on a stormy day. They have no control over anything they have earned. That is the utmost misguidance.', 'ta': 'எவர்கள் தங்களுடைய இறைவனை நிராகரிக்கிறார்களோ, அவர்களுக்கு உதாரணமாவது அவர்களுடைய செயல்கள் சாம்பல் போன்றவை புயல் காற்று கடினமாக வீசம் நாளில் அச்சாம்பலைக் காற்று அடித்துக் கொண்டு போய்விட்டது. (அவ்வாறே) தாங்கள் சம்பாதித்த பொருள்களில் எதன் மீதும் அவர்களுக்கு அதிகாரம் இராது இதுவே வெகு தூரமான வழிகேடாகும்.'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load English–Tamil parallel dataset\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ta\")\n",
    "\n",
    "# Inspect example\n",
    "print(dataset[\"train\"][0])\n",
    "# -> {'translation': {'en': 'Hello!', 'ta': 'வணக்கம்!'}}\n",
    "\n",
    "# Save Tamil sentences to .ta file\n",
    "with open(\"tamil_sentences.ta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in dataset[\"train\"]:\n",
    "        f.write(ex[\"translation\"][\"ta\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe319bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAMIL_UNICODE_RANGE = r'\\u0B80-\\u0BFF'\n",
    "ALLOWED_SYMBOLS = r'\\.,()\\[\\]{}'\n",
    "\n",
    "# Regex to remove unwanted characters\n",
    "remove_non_tamil = re.compile(fr'[^{TAMIL_UNICODE_RANGE}{ALLOWED_SYMBOLS}\\s]')\n",
    "\n",
    "# Regex to remove empty or non-Tamil-only brackets\n",
    "remove_bad_brackets = re.compile(\n",
    "    fr'''\n",
    "    [\\(\\[\\{{]               # opening bracket\n",
    "    [^{TAMIL_UNICODE_RANGE}]*  # anything but Tamil\n",
    "    [\\)\\]\\}}]               # closing bracket\n",
    "    ''', re.VERBOSE\n",
    ")\n",
    "\n",
    "with open(\"tamil_sentences.ta\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    # Remove unwanted non-Tamil characters (but keep allowed symbols)\n",
    "    cleaned_line = remove_non_tamil.sub('', line)\n",
    "\n",
    "    # Remove bad brackets like (), (,), {}, etc.\n",
    "    cleaned_line = remove_bad_brackets.sub('', cleaned_line)\n",
    "\n",
    "    cleaned_line = cleaned_line.strip()\n",
    "    if cleaned_line:\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "\n",
    "with open(\"tamil_sentences.ta\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in cleaned_lines:\n",
    "        outfile.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "107f669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/vijayabhaskar96/tamil-news-classification-dataset-tamilmurasu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "\n",
    "# Dataset identifier in \"owner/dataset-name\" format\n",
    "dataset = \"vijayabhaskar96/tamil-news-classification-dataset-tamilmurasu\"\n",
    "\n",
    "# Download to current working directory\n",
    "cwd = os.getcwd()\n",
    "api.dataset_download_files(dataset, path=cwd, unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcfe7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(\"tamilmurasu_dataset.csv\")\n",
    "\n",
    "# Select only the last three columns by name\n",
    "columns_to_extract = [\"news_category\", \"news_title\", \"news_article\"]\n",
    "df_selected = df[columns_to_extract]\n",
    "\n",
    "# Save the combined text of these columns into a .ta file, line by line\n",
    "with open(\"tamil_sentences-2.ta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df_selected.iterrows():\n",
    "        # Combine the three columns (you can customize the separator)\n",
    "        line = f\"{row['news_title']} | {row['news_article']} | {row['news_category']}\"\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c198333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Tamil Kavithai dataset\n",
    "ds = load_dataset(\"abishekmahi/tamil-kavithai\")\n",
    "\n",
    "with open(\"tamil_kavithai.ta\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in ds[\"train\"]:\n",
    "        content = example.get(\"Content\")\n",
    "        if content is not None:\n",
    "            f.write(content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b59390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Sequence, NFC, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import decoders\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f34e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token_dict = {\"unknown_token\": \"[UNK]\",\n",
    "                      \"pad_token\": \"[PAD]\", \n",
    "                      \"start_token\": \"[BOS]\",\n",
    "                      \"end_token\": \"[EOS]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6523af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(path_to_data_root):\n",
    "    \"\"\"\n",
    "    Train a WordPiece tokenizer specifically for Tamil text data.\n",
    "\n",
    "    Key settings:\n",
    "    - unknown_token: Used when the tokenizer encounters out-of-vocabulary words.\n",
    "    - pad_token: Used for padding Tamil sentences during batching.\n",
    "    - start_token: Prepended to Tamil input so the decoder knows where to start generation.\n",
    "    - end_token: Appended to the Tamil text to signal where decoding should stop.\n",
    "\n",
    "    Unicode Normalization:\n",
    "    Tamil letters can also have alternate Unicode representations (like combining forms).\n",
    "    NFC normalization ensures consistent representation using composed forms.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the tokenizer with WordPiece model and unknown token\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=special_token_dict[\"unknown_token\"]))\n",
    "\n",
    "    # Apply Unicode normalization and lowercase (optional for Tamil but kept for consistency)\n",
    "    tokenizer.normalizer = Sequence([NFC(), Lowercase()])\n",
    "\n",
    "    # Split text into tokens using whitespace\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Find all Tamil text files (.ta extension)\n",
    "    tamil_files = glob.glob(os.path.join(path_to_data_root, \"**/*.ta\"), recursive=True)\n",
    "\n",
    "    # Train a WordPiece tokenizer with vocab size and special tokens\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=32000,\n",
    "        special_tokens=list(special_token_dict.values())\n",
    "    )\n",
    "\n",
    "    # Train tokenizer on Tamil files\n",
    "    tokenizer.train(tamil_files, trainer)\n",
    "\n",
    "    # Save the trained tokenizer model\n",
    "    os.makedirs(\"trained_tokenizer\", exist_ok=True)\n",
    "    tokenizer.save(\"trained_tokenizer/tamil_wp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "276f488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_root = \".\"\n",
    "train_tokenizer(path_to_data_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
